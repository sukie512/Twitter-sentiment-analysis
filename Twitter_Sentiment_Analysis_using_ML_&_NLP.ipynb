{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1091329,
          "sourceType": "datasetVersion",
          "datasetId": 609423
        }
      ],
      "dockerImageVersionId": 29867,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Twitter Sentiment Analysis using ML & NLP",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukie512/Twitter-sentiment-analysis/blob/main/Twitter_Sentiment_Analysis_using_ML_%26_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re    # for regular expressions\n",
        "import nltk  # for text manipulation\n",
        "import string #for string manipulations\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "trusted": false,
        "id": "U_wyaKlbCnbP"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "dTQlEOdzGrCT",
        "outputId": "5559516c-12d2-471c-f100-551d18f578f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1e85b18c-7f1e-458a-9e05-c87d31af1320\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1e85b18c-7f1e-458a-9e05-c87d31af1320\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving twitter_training.csv.zip to twitter_training.csv (2).zip\n",
            "Saving twitter.csv.zip to twitter.csv (2).zip\n",
            "User uploaded file \"twitter_training.csv (2).zip\" with length 3309016 bytes\n",
            "User uploaded file \"twitter.csv (2).zip\" with length 1291530 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip twitter.csv.zip\n",
        "!unzip twitter_training.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZdjS5zLHKlk",
        "outputId": "ae9466ff-bda4-49aa-c4a9-a3936cb863eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  twitter.csv.zip\n",
            "replace twitter.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train  = pd.read_csv('twitter.csv')\n",
        "test = pd.read_csv('twitter_training.csv', header=None)\n",
        "test.columns = ['id', 'category', 'sentiment', 'tweet']\n",
        "test = test.drop(columns=['category', 'sentiment'])"
      ],
      "metadata": {
        "trusted": false,
        "id": "PvtCPshZCnbQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1YQTp3dGn9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "trusted": false,
        "id": "pNUGcoMSCnbR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train[train['label']==0].head(10)"
      ],
      "metadata": {
        "trusted": false,
        "id": "N_fo81p3CnbR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train[train['label']==1].head(10)"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "VBSdeHCXCnbR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape,test.shape"
      ],
      "metadata": {
        "trusted": false,
        "id": "Os3w91MrCnbR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train['label'].value_counts()"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "aqgYhnrXCnbR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train['tweet'] = train['tweet'].astype(str)\n",
        "test['tweet'] = test['tweet'].astype(str)\n",
        "\n",
        "train_length = train['tweet'].str.len()\n",
        "test_length = test['tweet'].str.len()\n",
        "plt.hist(train_length,bins=20,label=\"train_tweets\")\n",
        "plt.hist(test_length,bins=20,label=\"test_tweets\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": false,
        "id": "JD50zoZ_CnbR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "??plt.hist"
      ],
      "metadata": {
        "trusted": false,
        "id": "He5lXsYWCnbS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "combi = pd.concat([train, test], ignore_index=True, sort=False)\n",
        "combi.shape"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "eIhUP277CnbS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_pattern(input_txt,pattern):\n",
        "    r = re.findall(pattern,input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i,'',input_txt)\n",
        "    return input_txt"
      ],
      "metadata": {
        "trusted": false,
        "id": "su-KuBajCnbS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "combi.head()"
      ],
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "syTxqVoOCnbS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "?np.vectorize"
      ],
      "metadata": {
        "trusted": false,
        "id": "zPu9bVV0CnbS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#removing twitter handles @user\n",
        "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")\n",
        "combi.head()"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "9sRBbvi3CnbS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#removing punctuations , numbers and spl charecters\n",
        "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
        "combi.head()"
      ],
      "metadata": {
        "trusted": false,
        "id": "qemu2qEzCnbS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split()  if len(w)>3]))\n",
        "combi.head()"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "ng-R4OVWCnbS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\n",
        "tokenized_tweet.head()"
      ],
      "metadata": {
        "trusted": false,
        "id": "1sAbS3jTCnbS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "#stemming\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x:[stemmer.stem(i) for i in x])\n",
        "\n",
        "#Now let’s stitch these tokens back together. It can easily be done using nltk’s MosesDetokenizer function.\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "combi['tidy_tweet'] = tokenized_tweet"
      ],
      "metadata": {
        "trusted": false,
        "id": "jZcnoYrqCnbT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "KQ6uHtekCnbT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = ' '.join([text for text in combi['tidy_tweet']])\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "mtDepxP4CnbT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "Vi_J7xtFCnbX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "K3ObOhkyCnbX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# function to collect hashtags\n",
        "def hashtag_extract(x):\n",
        "    hashtags = []\n",
        "    # Loop over the words in the tweet\n",
        "    for i in x:\n",
        "        ht = re.findall(r\"#(\\w+)\", i)\n",
        "        hashtags.append(ht)\n",
        "    return hashtags"
      ],
      "metadata": {
        "trusted": false,
        "id": "vt8Ru5U-CnbX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting hashtags from non racist/sexist tweets\n",
        "HT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])\n",
        "# extracting hashtags from racist/sexist tweets\n",
        "HT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])\n",
        "# unnesting list\n",
        "HT_regular = sum(HT_regular,[])\n",
        "HT_negative = sum(HT_negative,[])"
      ],
      "metadata": {
        "trusted": false,
        "id": "3QCuZXadCnbX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "a = nltk.FreqDist(HT_regular)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()), 'Count': list(a.values())})\n",
        "# selecting top 20 most frequent hashtags\n",
        "d = d.nlargest(columns=\"Count\", n = 20)\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "O_dDuuppCnbX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "b = nltk.FreqDist(HT_negative)\n",
        "e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n",
        "# selecting top 20 most frequent hashtags\n",
        "e = e.nlargest(columns=\"Count\", n = 20)\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")"
      ],
      "metadata": {
        "trusted": false,
        "id": "u0-UZV4BCnbX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "PiCou6tYCnbX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "import gensim"
      ],
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "aHkIQYq3CnbY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])\n",
        "bow.shape"
      ],
      "metadata": {
        "trusted": false,
        "id": "lEy5jHdfCnbY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])\n",
        "tfidf.shape"
      ],
      "metadata": {
        "trusted": false,
        "id": "OmDka8zACnbY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\n",
        "# tokenizing\n",
        "model_w2v = gensim.models.Word2Vec(\n",
        "            tokenized_tweet,\n",
        "            size=200, # desired no. of features/independent variables\n",
        "            window=5, # context window size\n",
        "            min_count=2,\n",
        "            sg = 1, # 1 for skip-gram model\n",
        "            hs = 0,\n",
        "            negative = 10, # for negative sampling\n",
        "            workers= 2, # no.of cores\n",
        "            seed = 34)\n",
        "model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)\n"
      ],
      "metadata": {
        "trusted": false,
        "id": "TRqz_CJkCnbY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v.wv.most_similar(positive=\"dinner\")"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "PM1Kpid0CnbY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v.wv.most_similar(positive='trump')"
      ],
      "metadata": {
        "trusted": false,
        "id": "Fq-bsE1LCnbY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_w2v['food']"
      ],
      "metadata": {
        "trusted": false,
        "id": "FCMFZqkrCnbY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(model_w2v['food'])"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "tEGOuFhKCnbY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def word_vector(tokens,size):\n",
        "    vec = np.zeros(size).reshape((1,size))\n",
        "    count = 0\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec+=model_w2v[word].reshape((1,size))\n",
        "            count+=1.\n",
        "        except KeyError:\n",
        "            #handling the case where the token is not in vocabulary\n",
        "            continue\n",
        "    if count!=0:\n",
        "        vec/=count\n",
        "        return vec"
      ],
      "metadata": {
        "trusted": false,
        "id": "dbORmGYECnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
        "    wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "wordvec_df.shape"
      ],
      "metadata": {
        "trusted": false,
        "id": "1p7zIgmfCnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tqdm"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "P-CVT3fCCnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models.doc2vec import LabeledSentence"
      ],
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "zBbPU2XXCnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def add_label(twt):\n",
        "    output = []\n",
        "    for i, s in zip(twt.index, twt):\n",
        "        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n",
        "    return output\n",
        "\n",
        "labeled_tweets = add_label(tokenized_tweet) # label all the tweets"
      ],
      "metadata": {
        "trusted": false,
        "id": "lleXFja2CnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_tweets[:6]"
      ],
      "metadata": {
        "trusted": false,
        "id": "33ohkhr1CnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model\n",
        "                                  dm_mean=1, # dm = 1 for using mean of the context word vectors\n",
        "                                  size=200, # no. of desired features\n",
        "window=5, # width of the context window\n",
        "negative=7, # if > 0 then negative sampling will be used\n",
        "                                  min_count=5, # Ignores all words with total frequency lower than 2.\n",
        "workers=3, # no. of cores\n",
        "alpha=0.1, # learning rate\n",
        "seed = 23)\n",
        "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
        "\n",
        "model_d2v.train(labeled_tweets, total_examples= len(combi['tidy_tweet']), epochs=15)"
      ],
      "metadata": {
        "trusted": false,
        "id": "2UvITdL1CnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "docvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
        "for i in range(len(combi)):\n",
        "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))\n",
        "\n",
        "docvec_df = pd.DataFrame(docvec_arrays)\n",
        "docvec_df.shape"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "9FHKaNrwCnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "trusted": false,
        "id": "NGONrK0TCnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting train and test BoW features\n",
        "train_bow = bow[:31962,:]\n",
        "test_bow = bow[31962:,:]\n",
        "# splitting data into training and validation set\n",
        "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n",
        "\n",
        "lreg = LogisticRegression()\n",
        "# # training the model\n",
        "lreg.fit(xtrain_bow, ytrain)\n",
        "# prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
        "# prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
        "# prediction_int = prediction_int.astype(np.int)\n",
        "# f1_score(yvalid, prediction_int) # calculating f1 score for the validation set"
      ],
      "metadata": {
        "trusted": false,
        "id": "HddRGgTjCnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = lreg.predict_proba(test_bow)\n",
        "test_pred_int = test_pred[:,1] >= 0.3\n",
        "test_pred_int = test_pred_int.astype(np.int)\n",
        "test['label'] = test_pred_int\n",
        "submission = test[['id','label']]\n",
        "submission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file"
      ],
      "metadata": {
        "trusted": false,
        "id": "KHzLEzvFCnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_tfidf = tfidf[:31962,:]\n",
        "test_tfidf = tfidf[31962:,:]\n",
        "xtrain_tfidf = train_tfidf[ytrain.index]\n",
        "xvalid_tfidf = train_tfidf[yvalid.index]\n",
        "\n",
        "lreg.fit(xtrain_tfidf, ytrain)\n",
        "prediction = lreg.predict_proba(xvalid_tfidf)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "f1_score(yvalid, prediction_int) # calculating f1 score for the validation set"
      ],
      "metadata": {
        "trusted": false,
        "id": "T3uKXX-NCnbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_w2v = wordvec_df.iloc[:31962,:]\n",
        "test_w2v = wordvec_df.iloc[31962:,:]\n",
        "xtrain_w2v = train_w2v.iloc[ytrain.index,:]\n",
        "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n",
        "# lreg.fit(xtrain_w2v, ytrain)\n",
        "# prediction = lreg.predict_proba(xvalid_w2v)\n",
        "# prediction_int = prediction[:,1] >= 0.3\n",
        "# prediction_int = prediction_int.astype(np.int)\n",
        "# f1_score(yvalid, prediction_int)"
      ],
      "metadata": {
        "trusted": false,
        "id": "YFbYx9_pCnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "cj9tJFU7Cnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "trusted": false,
        "id": "YIpBHCBGCnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain)\n",
        "prediction = xgb_model.predict(xvalid_bow)\n",
        "f1_score(yvalid, prediction)"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "XXf6fKBxCnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = xgb_model.predict(test_bow)\n",
        "test['label'] = test_pred\n",
        "submission = test[['id','label']]\n",
        "submission.to_csv('sub_xgb_bow.csv', index=False)"
      ],
      "metadata": {
        "trusted": false,
        "id": "wYf5-svfCnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain)\n",
        "prediction = xgb.predict(xvalid_tfidf)\n",
        "f1_score(yvalid, prediction)"
      ],
      "metadata": {
        "trusted": false,
        "id": "2bR_qitJCnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain)\n",
        "prediction = xgb.predict(xvalid_w2v)\n",
        "f1_score(yvalid, prediction)"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "TYUtiyD-Cnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "dtrain = xgb.DMatrix(xtrain_w2v, label=ytrain)\n",
        "dvalid = xgb.DMatrix(xvalid_w2v, label=yvalid)\n",
        "dtest = xgb.DMatrix(test_w2v)\n",
        "# Parameters that we are going to tune\n",
        "params = {\n",
        "    'objective':'binary:logistic',\n",
        "    'max_depth':6,\n",
        "    'min_child_weight': 1,\n",
        "    'eta':.3,\n",
        "    'subsample': 1,\n",
        "    'colsample_bytree': 1\n",
        " }"
      ],
      "metadata": {
        "trusted": false,
        "id": "kw_yFRWGCnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_eval(preds, dtrain):\n",
        "    labels = dtrain.get_label().astype(np.int)\n",
        "    preds = (preds >= 0.3).astype(np.int)\n",
        "    return [('f1_score', f1_score(labels, preds))]"
      ],
      "metadata": {
        "trusted": false,
        "id": "p9MKcQqtCnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gridsearch_params = [\n",
        "    (max_depth, min_child_weight)\n",
        "    for max_depth in range(6,10)\n",
        "     for min_child_weight in range(5,8)\n",
        " ]\n",
        "\n",
        "max_f1 = 0. # initializing with 0\n",
        "best_params = None\n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "     # Update our parameters\n",
        "    params['max_depth'] = max_depth\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "\n",
        "     # Cross-validation\n",
        "    cv_results = xgb.cv(        params,\n",
        "        dtrain,        feval= custom_eval,\n",
        "        num_boost_round=200,\n",
        "        maximize=True,\n",
        "        seed=16,\n",
        "        nfold=5,\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "# Finding best F1 Score\n",
        "\n",
        "mean_f1 = cv_results['test-f1_score-mean'].max()\n",
        "\n",
        "boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
        "print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
        "if mean_f1 > max_f1:\n",
        "        max_f1 = mean_f1\n",
        "        best_params = (max_depth,min_child_weight)\n",
        "\n",
        "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))\n"
      ],
      "metadata": {
        "trusted": false,
        "id": "5OjSLxKCCnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Updating max_depth and min_child_weight parameters.\n",
        "params['max_depth'] = 9\n",
        "params['min_child_weight'] = 7"
      ],
      "metadata": {
        "trusted": false,
        "id": "KeEPzOv1Cnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gridsearch_params = [\n",
        "    (subsample, colsample)\n",
        "    for subsample in [i/10. for i in range(5,10)]\n",
        "    for colsample in [i/10. for i in range(5,10)] ]\n",
        "\n",
        "max_f1 = 0.\n",
        "best_params = None\n",
        "for subsample, colsample in gridsearch_params:\n",
        "    print(\"CV with subsample={}, colsample={}\".format(\n",
        "                             subsample,\n",
        "                             colsample))\n",
        "     # Update our parameters\n",
        "    params['colsample'] = colsample\n",
        "    params['subsample'] = subsample\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        feval= custom_eval,\n",
        "        num_boost_round=200,\n",
        "        maximize=True,\n",
        "        seed=16,\n",
        "        nfold=5,\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "     # Finding best F1 Score\n",
        "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
        "    boost_rounds = cv_results['test-f1_score-mean'].idxmax()\n",
        "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
        "    if mean_f1 > max_f1:\n",
        "        max_f1 = mean_f1\n",
        "        best_params = (subsample, colsample)\n",
        "\n",
        "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "xta7o7mECnba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "params['subsample'] = .9\n",
        "params['colsample_bytree'] = .5"
      ],
      "metadata": {
        "trusted": false,
        "id": "mOvokKaSCnbb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_f1 = 0.\n",
        "best_params = None\n",
        "for eta in [.3, .2, .1, .05, .01, .005]:\n",
        "    print(\"CV with eta={}\".format(eta))\n",
        "     # Update ETA\n",
        "    params['eta'] = eta\n",
        "\n",
        "     # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        feval= custom_eval,\n",
        "        num_boost_round=1000,\n",
        "        maximize=True,\n",
        "        seed=16,\n",
        "        nfold=5,\n",
        "        early_stopping_rounds=20\n",
        "    )\n",
        "\n",
        "     # Finding best F1 Score\n",
        "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
        "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
        "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
        "    if mean_f1 > max_f1:\n",
        "        max_f1 = mean_f1\n",
        "        best_params = eta\n",
        "print(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))"
      ],
      "metadata": {
        "trusted": false,
        "id": "kkImlHnjCnbb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Updating the learning rate\n",
        "params['eta'] = .2\n",
        "params\n",
        "{'colsample': 0.9,\n",
        " 'colsample_bytree': 0.5, 'eta': 0.1,\n",
        " 'max_depth': 8, 'min_child_weight': 6,\n",
        " 'objective': 'binary:logistic',\n",
        " 'subsample': 0.9}"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": false,
        "id": "OXHD8VV3Cnbb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    feval= custom_eval,\n",
        "    num_boost_round= 1000,\n",
        "    maximize=True,\n",
        "    evals=[(dvalid, \"Validation\")],\n",
        "    early_stopping_rounds=10\n",
        " )"
      ],
      "metadata": {
        "trusted": false,
        "id": "TqO39jLhCnbb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = xgb_model.predict(dtest)\n",
        "#test['label'] = (test_pred >= 0.3).astype(np.int)\n",
        "#submission = test[['id','label']]\n",
        "#submission.to_csv('sub_xgb_w2v_finetuned.csv', index=False)"
      ],
      "metadata": {
        "trusted": false,
        "id": "rq8me6PhCnbb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db24d619"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f68b42f"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b356edde"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}